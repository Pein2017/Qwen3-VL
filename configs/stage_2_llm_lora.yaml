extends: base.yaml

# Stage 2: LoRA on LLM, freeze ViT; fully train MLP aligner

model:
  model: output/stage_1_full_aligner_only/best/eff_batch_32-lr_1e-4/checkpoint-200

# Use model's native chat template; max length unified via base.yaml or global_max_length

template:
  max_pixels: 589824

prompts:
  scheme: B

tuner:
  train_type: lora
  use_swift_lora: false  # Use PEFT backend for better compatibility
  freeze_llm: false
  freeze_vit: true
  freeze_aligner: true  # Freeze aligner itself, but train it via modules_to_save
  target_modules: [all-linear]
  # CRITICAL: modules_to_save does NOT support ModuleList - must specify individual elements
  # Qwen3-VL has: model.visual.merger (single module) + deepstack_merger_list (ModuleList with 3 elements)
  modules_to_save:
    - model.visual.merger
    - model.visual.deepstack_merger_list.0
    - model.visual.deepstack_merger_list.1
    - model.visual.deepstack_merger_list.2
  lora_rank: 8
  lora_alpha: 32
  lora_dropout: 0.1
  lora_bias: none

training:
  output_dir: ./output/stage_2_llm_lora
  run_name: lora_8_32-lr_5e-4-eff_batch-32-
  add_version: true
  save_safetensors: true
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  packing: true

  save_only_model: true
  optimizer: multimodal
  learning_rate: 5.0e-4
  vit_lr: 2.0e-5
  aligner_lr: 5.0e-4
  num_train_epochs: 20

  gradient_accumulation_steps: 4

  # Eval & checkpointing
  eval_strategy: steps          # or epoch (ms-swift uses eval_strategy, not evaluation_strategy)
  save_strategy: best         
  eval_steps: 20               # cadence for eval (if steps)
  save_steps: 60              
  # load_best_model_at_end: true 
  metric_for_best_model: eval_loss
  greater_is_better: false
  save_total_limit: 2

  # Verbose logging
  logging_dir: ./tb/10-20
  logging_steps: 10
  logging_first_step: true

data:
  dataset_num_proc: 8
  dataloader_num_workers: 8
  eval_dataset: [placeholder]

deepspeed:
  enabled: true
  config: zero2

custom:
  train_jsonl: data/bbu_full_768/train.jsonl
  val_jsonl: data/bbu_full_768/val.jsonl
  augment_prob: 0.0
  images_per_user_turn: 2
  dump_conversation_text: false

extends: base.yaml

# Stage 3: LoRA on LAST-6 Vision blocks, LoRA on LLM last-2; aligner full-tuned

rlhf:
  rlhf_type: gkd
  teacher_model: model_cache/models/Qwen/Qwen3-VL-4B-Instruct
  beta: 0.04  # Light KL to let LLM LoRA adapt; raise (0.1-0.2) if language drifts too far
  sft_alpha: 1
  seq_kd: false
  lmbda: 0.0

model:
  # Point to stage 2 best checkpoint - will load base model + LLM LoRA
  model: output/10-29/stage_1-gkd/v3-20251029-135454/gkd-eff_batch_32-epoch_10/checkpoint-520

tuner:
  train_type: lora
  use_swift_lora: false
  freeze_llm: false     # ← Enable LoRA on LLM last-2 (see target_regex below)
  freeze_vit: false     # ← Add LoRA to LAST-6 Vision blocks
  freeze_aligner: true  # ← Keep aligner frozen (already full-tuned in stage 2)
  target_modules: [all-linear]
  # LLM last-2 + Vision last-4 blocks (20-23)
  target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:18|19|20|21|22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  
  # --- Alternative Configurations (choose ONE and replace target_regex above) ---
  # 4B: LLM last-4 + Vision last-4 (original default):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:20|21|22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 4B: LLM last-2 + Vision last-2:
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 4B: LLM last-6 + Vision last-6:
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:30|31|32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:18|19|20|21|22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 8B: LLM last-2 + Vision last-2 (Vision indices: 25|26):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:25|26)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 8B: LLM last-4 + Vision last-4 (Vision: 23|24|25|26):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:23|24|25|26)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 8B: LLM last-6 + Vision last-6 (Vision: 21|22|23|24|25|26):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:30|31|32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:21|22|23|24|25|26)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  modules_to_save:
    # Stage 3: Continue full-tuning aligner (optional - remove if you want to freeze it)
    - model.visual.merger
    - model.visual.deepstack_merger_list.0
    - model.visual.deepstack_merger_list.1
    - model.visual.deepstack_merger_list.2
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
  lora_bias: none

training:
  output_dir: ./output/10-30/stage_3_gkd
  run_name: gkd-last_6_4-epoch_50-eff_batch_32-ref_base_model-lan_kd_0.04-vision_kd_0.3-weaker_color_aug
  add_version: true
  save_safetensors: true
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  packing: true
  weight_decay: 0.1
  warmup_ratio: 0.2

  optimizer: multimodal
  learning_rate: 1.0e-4   
  vit_lr: 1.0e-4          
  aligner_lr: 5.0e-5     
  num_train_epochs: 50

  lr_scheduler_kwargs:
    min_lr: 1.0e-6

  gradient_accumulation_steps: 4

  # Eval & checkpointing
  eval_strategy: steps
  save_strategy: steps
  save_total_limit: 6
  eval_steps: 20
  save_steps: 500

  # Verbose logging
  logging_dir: ./tb/10-30/stage_3_gkd
  logging_steps: 10
  logging_first_step: true

data:
  dataset_num_proc: 8
  dataloader_num_workers: 16
  dataloader_pin_memory: true
  eval_dataset: [placeholder]

deepspeed:
  enabled: true
  config: zero2

custom:
  visual_kd:
    enabled: true
    weight: 0.3  # Heavier anchor on vision/aligner (0.1-0.2 typical). Lower if visuals overfit.
    targets: [merger, deepstack]
    distance: mse
    
  images_per_user_turn: 1
  trainer_variant: gkd_monitor
  dump_conversation_text: false
  # Augmentation pipeline (training only). Eval remains unchanged.
  augmentation:
    enabled: true
    bypass_prob: 0.2  # 20% of training samples remain clean (no augmentation)
    ops:
      # Geometric ops
      - name: hflip
        params: { prob: 0.5 }
      - name: vflip
        params: { prob: 0.2 }
      - name: rotate
        params: { max_deg: 25.0, prob: 0.4 }
      # Expand canvas to enclose rotated/scaled content (no information loss)
      - name: expand_to_fit_affine
        params: { multiple: 32 }
      # ✅ Zoom-IN focused (better for small objects) - always zoom in, never out
      # NOTE: scale zoom-in can cause visual-label misalignment (see random_crop/center_crop below)
      # - name: scale
      #   params: { lo: 1.1, hi: 1.4, prob: 0.25 }
      
      # ✅ Smart Random Crop with Label Filtering (for dense captioning)
      # - Filters objects based on visibility (min_coverage = 30%)
      # - Updates completeness field: "显示完整" → "只显示部分" if coverage < 95%
      # - Skips crop if < 4 objects remain or line objects present (preserves cable/fiber integrity)
      # - Perfect alignment: only describes visible objects
      - name: random_crop
        params:
          scale: [0.7, 1.0]               # Crop 70-100% of image
          aspect_ratio: [0.9, 1.1]        # Nearly square
          min_coverage: 0.3               # Drop objects <30% visible
          completeness_threshold: 0.95    # Mark "只显示部分" if <95% visible
          min_objects: 4                  # Skip crop if <4 objects (dense scenes)
          skip_if_line: true              # Skip crop if line objects present
          prob: 0.2                       # 30% of samples
      # ✅ Multi-scale training: prefer medium-large sizes (no aggressive shrinking)
      - name: resize_by_scale
        params: { lo: 0.9, hi: 1.2, align_multiple: 32, prob: 0.7 }
      # Color ops (reduced strength for less distortion)
      - name: color_jitter
        params: { brightness: [0.92, 1.08], contrast: [0.92, 1.08], saturation: [0.92, 1.08], prob: 0.3 }
      - name: gamma
        params: { gamma: [0.92, 1.1], prob: 0.15 }
      - name: hsv
        params: { hue_delta_deg: [-5, 5], sat: [0.95, 1.08], val: [0.95, 1.08], prob: 0.15 }
      - name: clahe
        params: { clip_limit: 1.5, tile_grid_size: [8, 8], prob: 0.1 }
      - name: auto_contrast
        params: { cutoff: 0, prob: 0.05 }
      # equalize removed - redundant with auto_contrast (both global histogram ops)
      - name: sharpness
        params: { factor: [0.9, 1.15], prob: 0.1 }
      # Note: pad_to_multiple removed - expand_to_fit_affine already handles it



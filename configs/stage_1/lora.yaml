extends: ../base.yaml

# Stage 1: LoRA training on Aligner only (freeze LLM+ViT) - WITH GKD TRAINER

model:
  model: model_cache/models/Qwen/Qwen3-VL-8B-Instruct

tuner:
  train_type: lora
  use_swift_lora: false
  freeze_llm: true
  freeze_vit: true
  freeze_aligner: false
  target_modules: [all-linear]
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
  lora_bias: none

rlhf:
  rlhf_type: gkd
  teacher_model: model_cache/models/Qwen/Qwen3-VL-8B-Instruct
  beta: 0.9
  sft_alpha: 1
  llm_kd_weight: 1e-8  # Scale LM-head KD; set to 0.0 to disable logits distillation.
  seq_kd: false
  lmbda: 0.0

training:
  # Paths and naming
  output_dir: ./output/11-17/stage_1
  run_name: eff_batch_32-epoch_6-lr_5e-4-lora_16_32
  
  # Epochs and batch sizes
  num_train_epochs: 6
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  # Auto-calculated from effective_batch_size, per_device_train_batch_size, and world_size
  # gradient_accumulation_steps: 8  # Commented out - now auto-calculated
  effective_batch_size: 32  # Will auto-calculate gradient_accumulation_steps to maintain this effective batch size
  
  # Optimizer and learning rates
  optimizer: multimodal
  learning_rate: 5.0e-4
  vit_lr: 2.0e-5
  aligner_lr: 5.0e-4  
  
  # Eval and checkpointing
  eval_strategy: steps
  save_strategy: best
  eval_steps: 20
  save_steps: 40
  save_total_limit: 3
  save_delay_steps: 300
  
  # Logging
  logging_dir: ./tb/11-17/stage_1
  logging_steps: 10
  logging_first_step: true

data:
  dataset_num_proc: 8
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4

custom:
  json_format: standard
  trainer_variant: gkd_monitor
  visual_kd:
    enabled: false
    vit:
      enabled: false
    aligner:
      enabled: true
      weight: 0.01  # Heavier anchor on vision/aligner (0.1-0.2 typical). Lower if visuals overfit.
      distance: mse
    deepstack:
      enabled: false
  emit_norm: norm1000
  dump_conversation_text: false

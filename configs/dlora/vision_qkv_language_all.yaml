extends: ./sft_base.yaml

# Variant: tune all LLM linear layers and all ViT attention QKV (no vision FFN).
# Overrides sft_base.yaml's full-dlora with specific target_regex.

tuner:
  # All language tower linears + every vision block attn.qkv (0-26), keep aligner MLPs.
  target_regex: '^(?:model\.)?(?:language_model\.layers\.\d+\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:[0-9]|1[0-9]|2[0-6])\.attn\.qkv|visual\.(?:merger|deepstack_merger_list\.\d+)\.(?:linear_fc1|linear_fc2))$'

training:
  learning_rate: 2.0e-4
  vit_lr: 1.0e-4
  aligner_lr: 4.0e-4
  run_name: epoch_30-dlora-lrs_2_1_4-sft_vision_qkv

extends: ./sft_base.yaml

# Vision last-6 blocks: attn.qkv only (no MLP)
# DoRA on language tower (q_proj & v_proj), last-6 Vision blocks (21-26, attn.qkv only), aligner MLP layers.
# Overrides sft_base.yaml's full-dlora with specific target_regex.

tuner:
  target_regex: '^(?:model\.)?(?:language_model\.layers\.\d+\.self_attn\.(?:q_proj|v_proj)|visual\.blocks\.(?:21|22|23|24|25|26)\.attn\.qkv|visual\.(?:merger|deepstack_merger_list\.\d+)\.(?:linear_fc1|linear_fc2))$'

training:
  run_name: epoch_30-dlora-lrs_4_2_8-vision_last6_attn_only


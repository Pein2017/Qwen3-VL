extends: ../base.yaml

global_max_length: 4096

# Common Stage 1.5 recipe: DoRA on all LLM layers (q_proj & v_proj only),
# DoRA on last-6 Vision blocks (21-26, qkv only), DoRA on all aligner MLP layers.

model:
  model: model_cache/models/Qwen/Qwen3-VL-8B-Instruct  # Stage 2 checkpoint (base + LLM LoRA)

tuner:
  train_type: lora
  use_swift_lora: false
  use_dora: true
  freeze_llm: false
  freeze_vit: false
  freeze_aligner: false
  target_modules: [all-linear]
  target_regex: null
  modules_to_save: []
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
  lora_bias: none

training:
  # Override per experiment; baseline single-target values provided here.
  output_dir: ./output/11-24/stage_1.5
  run_name: epoch_30-dlora-lrs_4_2_8-sft_all
  num_train_epochs: 30
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  effective_batch_size: 32
  optimizer: multimodal
  # LR groups (LLM/ViT/aligner) â€” tune per experiment as needed.
  learning_rate: 4.0e-4
  vit_lr: 2.0e-4
  aligner_lr: 8.0e-4
  lr_scheduler_type: cosine_warmup_with_min_lr
  lr_scheduler_kwargs:
    min_lr: 1.0e-6
  warmup_ratio: 0.1
  gradient_checkpointing: true
  bf16: true
  report_to: [tensorboard]
  seed: 17
  max_grad_norm: 0.5
  weight_decay: 0.0
  packing: false
  add_version: true
  save_safetensors: true
  eval_strategy: steps
  eval_steps: 40
  save_strategy: best
  save_steps: 10
  save_total_limit: 2
  save_last_epoch: true
  save_only_model: true
  metric_for_best_model: eval_token_acc
  greater_is_better: true
  save_delay_steps: 1800
  logging_dir: ./tb/11-24/stage_1.5
  logging_steps: 10
  logging_first_step: true

data:
  # Required by ms-swift TrainArguments validation (actual datasets loaded via custom.train_jsonl/val_jsonl)
  dataset: ["dummy"]
  val_dataset: ["dummy"]  # Prevent eval_strategy from being overridden to 'no'
  dataset_num_proc: 8
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2
  dataloader_persistent_workers: true

deepspeed:
  enabled: true
  config: zero2

custom:
  json_format: standard
  emit_norm: norm1000
  train_jsonl: data/bbu_full_768_poly-need_review/train.jsonl
  val_jsonl: data/bbu_full_768_poly-need_review/val.jsonl
  dump_conversation_path: output/conversation_text.txt
  # Pure BBU (no fusion); downstream configs enable fusion by overriding this field.
  fusion_config: null
  token_type_metrics:
    enabled: true
    include: [bbu, rru, lvis]
    exclude: [lang_chat]
  augmentation:
    enabled: true
    bypass_prob: 0.1
    ops:
      - name: hflip
        params: { prob: 0.2 }
      - name: vflip
        params: { prob: 0.05 }
      - name: rotate
        params: { max_deg: 12.0, prob: 0.18 }
      - name: expand_to_fit_affine
        params: { multiple: 32 }
      - name: random_crop
        params:
          scale: [0.88, 1.0]
          aspect_ratio: [0.95, 1.05]
          min_coverage: 0.4
          completeness_threshold: 0.95
          min_objects: 4
          skip_if_line: true
          prob: 0.10
      - name: small_object_zoom_paste  # fixed strength across training
        params:
          prob: 0.14
          max_targets: 4
          max_attempts: 20
          scale: [1.0, 1.5]
          max_size: 96
          max_line_length: 128
          overlap_threshold: 0.1
          context: 4
          line_buffer: 4
      - name: resize_by_scale
        params: { lo: 0.95, hi: 1.08, align_multiple: 32, prob: 0.34 }
      - name: color_jitter
        params: { brightness: [0.95, 1.05], contrast: [0.95, 1.05], saturation: [0.95, 1.05], prob: 0.15 }
      - name: gamma
        params: { gamma: [0.95, 1.05], prob: 0.08 }
      - name: hsv
        params: { hue_delta_deg: [-3, 3], sat: [0.98, 1.02], val: [0.98, 1.02], prob: 0.08 }
      - name: clahe
        params: { clip_limit: 1.5, tile_grid_size: [8, 8], prob: 0.05 }
      - name: auto_contrast
        params: { cutoff: 0, prob: 0.04 }
      - name: sharpness
        params: { factor: [0.95, 1.08], prob: 0.08 }

  augmentation_curriculum:
    phases:
      - until_percent: 5
        bypass_prob: 1.0
        ops: {}
      - until_percent: 20
        bypass_prob: 0.30
        ops:
          rotate: { prob: 0.14, max_deg: 9.0 }
          random_crop: { prob: 0.07, scale: [0.93, 1.00] }
          resize_by_scale: { prob: 0.26, lo: 0.97, hi: 1.04 }
          color_jitter: { prob: 0.10, brightness: [0.97, 1.03], contrast: [0.97, 1.03], saturation: [0.97, 1.03] }
      - until_percent: 45
        bypass_prob: 0.22
        ops:
          rotate: { prob: 0.18, max_deg: 11.0 }
          random_crop: { prob: 0.10, scale: [0.90, 0.99] }
          resize_by_scale: { prob: 0.33, lo: 0.96, hi: 1.06 }
          color_jitter: { prob: 0.14, brightness: [0.95, 1.05], contrast: [0.95, 1.05], saturation: [0.95, 1.05] }
      - until_percent: 70
        bypass_prob: 0.15
        ops:
          rotate: { prob: 0.22, max_deg: 13.0 }
          random_crop: { prob: 0.12, scale: [0.88, 0.98] }
          resize_by_scale: { prob: 0.38, lo: 0.95, hi: 1.07 }
          color_jitter: { prob: 0.18, brightness: [0.93, 1.07], contrast: [0.93, 1.07], saturation: [0.93, 1.07] }
          hsv: { prob: 0.06, hue_delta_deg: [-3, 3], sat: [0.98, 1.02], val: [0.98, 1.02] }
      - until_percent: 100
        bypass_prob: 0.08
        ops:
          rotate: { prob: 0.26, max_deg: 15.0 }
          random_crop: { prob: 0.14, scale: [0.86, 0.98] }
          resize_by_scale: { prob: 0.42, lo: 0.94, hi: 1.08 }
          color_jitter: { prob: 0.22, brightness: [0.92, 1.08], contrast: [0.92, 1.08], saturation: [0.92, 1.08] }
          hsv: { prob: 0.08, hue_delta_deg: [-4, 4], sat: [0.97, 1.03], val: [0.97, 1.03] }

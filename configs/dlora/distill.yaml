extends: ./sft_base.yaml

# Stage-B verdict distillation SFT (text-only ChatML).
#
# Expected data preparation:
# - Collect per-mission distill exports and split into `data/stage_b/distill.train.jsonl`
#   and `data/stage_b/distill.val.jsonl` (see `scripts/stage_b_split_distill.py`).
# - Each record must include: {"messages": [...]} where the last assistant turn is:
#     Verdict: 通过|不通过
#     Reason: ...
#
# Notes:
# - This run is intentionally LoRA-only on the LLM tower (vision + aligner frozen).
# - Keep `global_max_length` large enough for long Stage-B prompts (guidance + per-image summaries).

global_max_length: 12000

model:
  # Set to the current "summary" checkpoint that drifted; distillation fine-tunes verdict behavior on top.
  # Example (update to the real checkpoint used in production):
  model: output/12-23/summary_merged/epoch_2-bbu_rru-more_irrelevant-ocr

tuner:
  # Freeze multimodal components; adapt only the LLM via LoRA/DoRA.
  freeze_llm: false
  freeze_vit: true
  freeze_aligner: true
  # Keep lm_head fixed for stability (no modules_to_save).
  modules_to_save: []
  lora_rank: 16
  lora_alpha: 32

training:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  effective_batch_size: 32
  num_train_epochs: 2
  output_dir: ./output/12-25/stageb_distill
  logging_dir: ./tb/12-25/stageb_distill
  run_name: epoch_2-stageb_distill-verdict
  # Conservative LR to avoid overwriting summary/dense skills while restoring Stage-B verdict behavior.
  learning_rate: 5.0e-5
  vit_lr: 1.0e-7
  aligner_lr: 1.0e-7
  save_delay_steps: 200

custom:
  # Dummy base path for ROOT_IMAGE_DIR auto-derivation; for fusion runs this is not used as the dataset.
  train_jsonl: data/stage_b/distill.train.jsonl
  val_jsonl: data/stage_b/distill.val.jsonl
  fusion_config: configs/fusion/stage_b_distill.yaml
  use_summary: false
  dump_conversation_text: false
  augmentation:
    enabled: false
  augmentation_curriculum: null

stage_a_paths:
  - output_post/stage_a/挡风板安装检查_stage_a_tiny.jsonl

seed: 17

model:
  model_name_or_path: output/12-2/fusion_dlora_merged/checkpoint-4650
  torch_dtype: bfloat16
  device_map: auto

guidance:
  path: output_post/stage_b/initial_guidance.json
  retention: 5

output:
  root: output_post/stage_b
  run_name: debug-0.1-0.3-0.5
  group_report: true

sampler:
  samples_per_decode: 1
  format_filter: true
  grid:
    - temperature: 0.001
      top_p: 0.9
      max_new_tokens: 1024
      seed: 42
      repetition_penalty: 1.05
      stop:
        - "assistant"
        - "<|im_end|>"
        - "<|endoftext|>"
        - "</s>"
    - temperature: 0.3
      top_p: 0.9
      max_new_tokens: 1024
      seed: 43
      repetition_penalty: 1.05
      stop:
        - "assistant"
        - "<|im_end|>"
        - "<|endoftext|>"
        - "</s>"
    - temperature: 0.5
      top_p: 0.9
      max_new_tokens: 1024
      seed: null
      repetition_penalty: 1.05
      stop:
        - "assistant"
        - "<|im_end|>"
        - "<|endoftext|>"
        - "</s>"


reflection:
  prompt_path: configs/prompts/stage_b_reflection_prompt.txt
  batch_size: 8  # Debug: 保持较小批次，便于观察每次反思
  max_operations: 3  # Debug: 每次最多 3 条经验修改

  temperature: 0.001  # Higher temperature for more diverse guidance entries
  top_p: 0.9  # Higher top_p for more diverse sampling
  repetition_penalty: 1.05
  max_new_tokens: 2048  # Allow longer responses to avoid truncation (G1 case)
  max_reflection_length: 12000  # Larger packed context
  token_budget: 12000  # Allow larger packed context to avoid dropping records
  require_rule_for_conflicts: true
  treat_keep_conflict_as_noise: false  # Do not auto-mark keep-on-conflict as noise; rely on conflict rules instead

selection:
  policy: top_label
  tie_break: temperature


manual_review:
  enabled: true
  min_verdict_agreement: 0.8  # fraction of candidates sharing the same verdict
  min_self_consistency: 0.8   # average self_consistency for majority verdict

runner:
  epochs: 2 # Tiny smoke: 单轮
  rollout_batch_size: 1  

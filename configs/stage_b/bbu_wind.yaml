stage_a_paths:
  - output_post/stage_a_bbu_rru_summary/挡风板安装检查_stage_a.jsonl
seed: 17
default_domain: bbu

model:
  model_name_or_path: model_cache/models/Qwen/Qwen3-VL-8B-Instruct
  torch_dtype: bfloat16
  device_map: auto
  attn_implementation: flash_attention_2

guidance:
  path: output_post/stage_b/initial_guidance.json
  retention: 5
  reset_on_rerun: true

output:
  root: output_post/stage_b
  run_name: 12-18-rule-search-reflect-16-train-eval

rule_search:
  proposer_prompt_path: configs/prompts/stage_b_rule_search_proposer_prompt.txt
  reflect_size: 16
  num_candidate_rules: 3

  train_pool_size: 512
  eval_pool_fraction: 0.1

  gate:
    min_relative_error_reduction: 0.1
    max_changed_fraction: 0.05
    max_fp_rate_increase: 0.01
    bootstrap:
      iterations: 200
      min_prob: 0.8
      seed: 17

  early_stop:
    patience: 3

  # Train pool uses multi-sample decoding for majority vote.
  train_sampler:
    samples_per_decode: 5
    grid:
      - temperature: 0.1
        top_p: 0.9
        max_new_tokens: 1024
        seed: 43
        repetition_penalty: 1.05
  # Eval pool uses low-temp single-sample decoding.
  eval_sampler:
    samples_per_decode: 1
    grid:
      - temperature: 0.1
        top_p: 0.9
        max_new_tokens: 1024
        seed: 43
        repetition_penalty: 1.05

# Reflection block is required by schema (used for proposer in rule_search mode).
reflection:
  decision_prompt_path: configs/prompts/stage_b_reflection_decision_prompt.txt
  ops_prompt_path: configs/prompts/stage_b_reflection_ops_prompt.txt
  batch_size: 16
  max_operations: 2
  temperature: 0.1
  top_p: 0.9
  repetition_penalty: 1.05
  max_new_tokens: 2048
  max_reflection_length: 4096
  token_budget: 18000

runner:
  epochs: 50
  per_rank_rollout_batch_size: 32
  logging_steps: 64

stage_b_distillation:
  enabled: false

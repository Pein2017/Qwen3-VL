stage_a_paths:
  - output_post/stage_a/挡风板安装检查_stage_a.jsonl
  # /output_post/stage_a/BBU接地线检查_stage_a.jsonl

seed: 17

model:
  model_name_or_path: output_4b/summary_merged/10-31
  torch_dtype: bfloat16
  device_map: auto

guidance:
  path: output_post/stage_b/initial_guidance.json
  retention: 5

output:
  root: output_post/stage_b
  run_name: debug
  group_report: true

sampler:
  samples_per_decode: 2
  format_filter: true
  grid:
    - temperature: 0.1
      top_p: 0.9
      max_new_tokens: 128
      seed: 42
      no_repeat_ngram_size: 6
      stop:
        - "assistant"
        - "<|im_end|>"
        - "<|endoftext|>"
        - "</s>"
      repetition_penalty: 1.05
    - temperature: 0.2
      top_p: 0.9
      max_new_tokens: 128
      seed: 43
      no_repeat_ngram_size: 6
      stop:
        - "assistant"
        - "<|im_end|>"
        - "<|endoftext|>"
        - "</s>"
      repetition_penalty: 1.05
    - temperature: 0.3
      top_p: 0.9
      max_new_tokens: 128
      seed: null
      no_repeat_ngram_size: 6
      stop:
        - "assistant"
        - "<|im_end|>"
        - "<|endoftext|>"
        - "</s>"
      repetition_penalty: 1.05

reflection:
  prompt_path: configs/prompts/stage_b_reflection_prompt.txt
  batch_size: 8
  max_operations: 3

  temperature: 0.3  # Higher temperature for more diverse guidance entries
  top_p: 0.9  # Higher top_p for more diverse sampling
  repetition_penalty: 1.05
  max_new_tokens: 2560  # Allow longer responses for multiple guidance entries
  max_reflection_length: 12000  # Max input length for reflection prompts
  token_budget: 12000
  require_rule_for_conflicts: true
  treat_keep_conflict_as_noise: false

selection:
  policy: top_label
  tie_break: temperature

runner:
  epochs: 1
  rollout_batch_size: 16  # 批量生成提速；可按显存调整

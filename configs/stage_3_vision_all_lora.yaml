extends: base.yaml

# Stage 3: LoRA on ALL Vision blocks, LoRA on LLM last-2; aligner full-tuned

model:
  # Point to stage 2 best checkpoint - will load base model + LLM LoRA
  model: output/stage_2_merged-10-25

prompts:
  scheme: B

tuner:
  train_type: lora
  use_swift_lora: false
  freeze_llm: false     # ← Enable LoRA on LLM last-2 (see target_regex below)
  freeze_vit: false     # ← Add LoRA to ALL Vision blocks
  freeze_aligner: true  # ← Keep aligner frozen (already full-tuned in stage 2)
  target_modules: [all-linear]
  # LLM last-2 + Vision ALL blocks
  target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  
  # --- Alternative Configurations (choose ONE and replace target_regex above) ---
  # 4B: LLM last-4 + Vision last-4 (original default):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:20|21|22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 4B: LLM last-2 + Vision last-2:
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 4B: LLM last-6 + Vision last-6:
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:30|31|32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:18|19|20|21|22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 8B: LLM last-2 + Vision last-2 (Vision indices: 25|26):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:25|26)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 8B: LLM last-4 + Vision last-4 (Vision: 23|24|25|26):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:23|24|25|26)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 8B: LLM last-6 + Vision last-6 (Vision: 21|22|23|24|25|26):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:30|31|32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:21|22|23|24|25|26)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  modules_to_save:
    # Stage 3: Continue full-tuning aligner (optional - remove if you want to freeze it)
    - model.visual.merger
    - model.visual.deepstack_merger_list.0
    - model.visual.deepstack_merger_list.1
    - model.visual.deepstack_merger_list.2
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
  lora_bias: none

training:
  output_dir: ./output/10-26/stage_3
  run_name: all_lora-epoch_50-eff_batch_16
  add_version: true
  save_safetensors: true
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  packing: true
  weight_decay: 0.1
  warmup_ratio: 0.2

  optimizer: multimodal
  learning_rate: 1.0e-4   # Vision LoRA learning rate
  vit_lr: 1.5e-4          # Higher LR for LoRA on ALL vision blocks
  aligner_lr: 1.0e-4      # Aligner continues training (if in modules_to_save)
  num_train_epochs: 50

  lr_scheduler_kwargs:
    min_lr: 1.0e-6

  gradient_accumulation_steps: 2

  # Eval & checkpointing
  eval_strategy: steps
  save_strategy: best
  eval_steps: 20
  save_steps: 40
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  save_total_limit: 2

  # Verbose logging
  logging_dir: ./tb/10-26
  logging_steps: 10
  logging_first_step: true

data:
  dataset_num_proc: 8
  dataloader_num_workers: 16
  dataloader_pin_memory: true
  eval_dataset: [placeholder]

deepspeed:
  enabled: true
  config: zero2

custom:
  images_per_user_turn: 1
  dump_conversation_text: false
  # Augmentation pipeline (training only). Eval remains unchanged.
  augmentation:
    enabled: true
    ops:
      # Geometric ops
      - name: hflip
        params: { prob: 0.5 }
      - name: vflip
        params: { prob: 0.2 }
      - name: rotate
        params: { max_deg: 25.0, prob: 0.4 }
      # ✅ Zoom-IN focused (better for small objects) - always zoom in, never out
      - name: scale
        params: { lo: 1.1, hi: 1.4, prob: 0.25 }
      # ✅ Multi-scale training: prefer medium-large sizes (no aggressive shrinking)
      - name: resize_by_scale
        params: { lo: 0.75, hi: 1.2, align_multiple: 32, prob: 0.7 }
      # Color ops (medium strength default)
      - name: color_jitter
        params: { brightness: [0.75, 1.25], contrast: [0.75, 1.25], saturation: [0.75, 1.25], prob: 0.5 }
      - name: gamma
        params: { gamma: [0.8, 1.3], prob: 0.3 }
      - name: hsv
        params: { hue_delta_deg: [-15, 15], sat: [0.8, 1.3], val: [0.8, 1.3], prob: 0.3 }
      - name: clahe
        params: { clip_limit: 3.0, tile_grid_size: [8, 8], prob: 0.15 }
      - name: auto_contrast
        params: { cutoff: 0, prob: 0.1 }
      - name: equalize
        params: { prob: 0.08 }
      - name: sharpness
        params: { factor: [0.7, 1.4], prob: 0.25 }
      # Enforce size multiple for Qwen3-VL vision grid (pad only; keeps pixel coords)
      - name: pad_to_multiple
        params: { multiple: 32 }


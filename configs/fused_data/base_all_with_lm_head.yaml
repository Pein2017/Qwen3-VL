extends: base_all.yaml

# Variant that also applies LoRA/DoRA to the lm_head in addition to all linear layers
# by matching module names directly (avoids ParamWrapper/target_parameters path).
tuner:
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - up_proj
    - gate_proj
    - down_proj
    - qkv               # Vision fused attention projection
    - proj              # Vision/attention projection heads, patch_embed.proj
    - linear_fc1        # Vision MLP
    - linear_fc2        # Vision MLP
    - lm_head           # Language model head
  target_parameters: []

training:
  run_name: epoch_50-lrs_2_1_6-all_linear_plus_lm_head-bs_32

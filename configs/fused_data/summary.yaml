extends: ./sft_base.yaml

model:
  model: output/12-9/res_1024_fusion_merged/epoch50-updated_aug-checkpoint-4650

tuner:
  # Freeze vision/aligner; LoRA only on last-2 LLM blocks (lm_head fixed)
  freeze_llm: false   # Enable LoRA adapters on selected LLM layers
  freeze_vit: true    # Freeze vision encoder
  freeze_aligner: false  # Freeze MLP aligner
  # LoRA targets: LLM layers 34-35 (q/k/v/o + MLP) excluding lm_head
  target_regex: null
  modules_to_save: []  # Keep lm_head fixed (no tuning/saving)
  lora_rank: 16
  lora_alpha: 32

training:
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  effective_batch_size: 32
  num_train_epochs: 2
  output_dir: ./output/12-18/summary
  logging_dir: ./tb/12-18/summary
  run_name: epoch_2-res_1024-bbu_rru_fused-lrs_1e-4_mlp_1e-6_with_irrelevant_summary-aug_off-with_rru_summary
  learning_rate: 1.0e-4
  vit_lr: 1.0e-7
  aligner_lr: 1.0e-6
  save_delay_steps: 300
  
custom:
  # train_jsonl: data/bbu_full_768_poly-need_review/train.jsonl
  # val_jsonl: data/bbu_full_768_poly-need_review/val.jsonl
  fusion_config: configs/fusion/summary_lang_chat_0p2.yaml
  emit_norm: norm1000
  # Default to summary; individual fusion datasets can override with mode
  use_summary: true
  # Collapse identifiable 标签/* entries into 标签/可以识别 while keeping 无法识别 separate
  summary_label_grouping: true
  dump_conversation_text: false
  # Disable augmentation completely (images are pre-processed offline)
  # Images are pre-processed with smart_resize + pad_to_multiple in build_irrelevant_summary_jsonl.py
  augmentation: null
  augmentation_curriculum: null

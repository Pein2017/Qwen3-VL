# Base Configuration - Shared defaults for Qwen3-VL SFT

global_max_length: 10000

model:
  model: model_cache/models/Qwen/Qwen3-VL-4B-Instruct
  torch_dtype: bfloat16
  attn_impl: flash_attention_2

template:
  template: qwen3_vl
  truncation_strategy: right
  max_pixels: 401408

tuner:
  train_type: lora
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: [all-linear]
  freeze_llm: true
  freeze_vit: true
  freeze_aligner: false

training:
  # defaults, can be overridden per-environment
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_checkpointing: true
  bf16: true
  packing: true
  report_to: [tensorboard]
  seed: 17

data:
  dataset: [placeholder]

deepspeed:
  enabled: true

custom:
  emit_norm: norm1000
  images_per_user_turn: 2

prompts:
  scheme: B



# Base Configuration - Shared defaults for Qwen3-VL SFT

global_max_length: 20000

model:
  torch_dtype: bfloat16
  attn_impl: flash_attention_2

template:
  template: qwen3_vl
  truncation_strategy: right

deepspeed:
  enabled: true
  config: zero2

training:
  # defaults, can be overridden per-environment
  gradient_checkpointing: true
  lr_scheduler_type: cosine_warmup_with_min_lr
  bf16: true
  report_to: [tensorboard]
  seed: 17
  warmup_ratio: 0.1
  max_grad_norm: 0.5
  weight_decay: 0.1 
  
  # Common trainer/runtime defaults
  add_version: true
  save_safetensors: true
  packing: true
  
  # Eval & checkpointing defaults
  eval_strategy: steps
  eval_steps: 20
  save_strategy: best
  save_steps: 40
  save_total_limit: 2
  metric_for_best_model: eval_loss
  greater_is_better: false
  save_delay_steps: 100  # Prevent checkpoint saves during first 100 steps (warmup protection)
  
  # Logging
  logging_steps: 10
  logging_first_step: true
  
  # Scheduler extras
  lr_scheduler_kwargs:
    min_lr: 1.0e-6

data:
  dataset: [placeholder]
  dataset_num_proc: 8
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4
  eval_dataset: [placeholder]

deepspeed:
  enabled: true
  config: zero2

custom:
  emit_norm: norm1000
  train_jsonl: data/bbu_full_768/train.jsonl
  val_jsonl: data/bbu_full_768/val.jsonl
  dump_conversation_path: output/conversation_text.txt

template:
  max_pixels: 1572864999  # an extremely large value so that never `do_resize`


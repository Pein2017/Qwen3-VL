extends: base.yaml

# Stage 3: LoRA on Vision, freeze LLM; keep aligner full-tuned from stage 2

model:
  # Point to stage 2 best checkpoint - will load base model + LLM LoRA
  model: output/stage_1_full_aligner_only/v3-20251021-022419/eff_batch_16-lr_5e-4-epoch_10/checkpoint-1100

# Use model's native chat template; max length unified via base.yaml or global_max_length

template:
  max_pixels: 401408

prompts:
  scheme: B

tuner:
  train_type: lora
  use_swift_lora: false
  freeze_llm: false     # ← Enable LoRA on LLM last-K layers (see target_regex below)
  freeze_vit: false     # ← Add LoRA to Vision blocks
  freeze_aligner: true  # ← Keep aligner frozen (already full-tuned in stage 2)
  target_modules: [all-linear]
  # By default, apply LoRA to last 4 blocks of LLM and Vision for Qwen3-VL-4B
  #  - LLM last-4 layers (shared 4B/8B): layers 32|33|34|35
  #  - Vision last-4 blocks (4B): blocks 20|21|22|23
  # For Qwen3-VL-8B, use the commented alternatives below.
  target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:20|21|22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # --- Alternatives (choose ONE and replace target_regex above) ---
  # 4B last-2:
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 4B last-6:
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:30|31|32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:18|19|20|21|22|23)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 8B last-2 (Vision indices differ: 25|26):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:25|26)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 8B last-4 (Vision 23|24|25|26):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:23|24|25|26)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  # 8B last-6 (Vision 21|22|23|24|25|26):
  # target_regex: '^(?:model\.)?(?:language_model\.layers\.(?:30|31|32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))|visual\.blocks\.(?:21|22|23|24|25|26)\.(?:attn\.(?:qkv|proj)|mlp\.(?:linear_fc1|linear_fc2)))$'
  modules_to_save:
    # Stage 3: Continue full-tuning aligner (optional - remove if you want to freeze it)
    - model.visual.merger
    - model.visual.deepstack_merger_list.0
    - model.visual.deepstack_merger_list.1
    - model.visual.deepstack_merger_list.2
  lora_rank: 8
  lora_alpha: 32
  lora_dropout: 0.1
  lora_bias: none

training:
  output_dir: ./output/stage_3_vision_llm_loRA
  run_name: lora_8_32-eff_batch-32
  add_version: true
  save_safetensors: true
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  packing: true

  optimizer: multimodal
  learning_rate: 8.0e-4   # Vision LoRA learning rate
  vit_lr: 8.0e-4          # Same as learning_rate for vision
  aligner_lr: 1.0e-5      # Aligner continues training (if in modules_to_save)
  num_train_epochs: 10

  gradient_accumulation_steps: 2

  # Eval & checkpointing
  eval_strategy: steps
  save_strategy: best
  eval_steps: 20
  save_steps: 40
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  save_total_limit: 2

  # Verbose logging
  logging_dir: ./tb/10-21/vision_lora
  logging_steps: 10
  logging_first_step: true

data:
  dataset_num_proc: 8
  dataloader_num_workers: 8
  eval_dataset: [placeholder]

deepspeed:
  enabled: true
  config: zero2

custom:
  train_jsonl: data/ds_v2_full/train.jsonl
  val_jsonl: data/ds_v2_full/val.jsonl
  augment_prob: 0.0
  images_per_user_turn: 4
  dump_conversation_text: false


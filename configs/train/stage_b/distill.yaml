extends:
  - ../../base.yaml
  - ../../components/global/max_length_12000.yaml
  - ../../components/model/qwen3_vl_4b.yaml
  - ../../components/tuner/dora_full.yaml
  - ../../components/tuner/stage_b_distill.yaml
  - ../../components/training/sft_defaults.yaml
  - ../../components/data/loader_defaults.yaml
  - ../../components/custom/common_fusion.yaml
  - ../../components/custom/augmentation_disabled.yaml
  - ../../components/deepspeed/zero2.yaml

# Stage-B verdict distillation SFT (text-only ChatML).
#
# Expected data preparation:
# - Collect per-mission distill exports and split into `data/stage_b/distill.train.jsonl`
#   and `data/stage_b/distill.val.jsonl` (see `scripts/stage_b_split_distill.py`).
# - Each record must include: {"messages": [...]} where the last assistant turn is:
#     Verdict: 通过|不通过
#     Reason: ...

model:
  # Set to the current "summary" checkpoint that drifted; distillation fine-tunes verdict behavior on top.
  # Example (update to the real checkpoint used in production):
  model: output/12-23/summary_merged/epoch_2-bbu_rru-more_irrelevant-ocr

training:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  effective_batch_size: 32
  num_train_epochs: 2
  output_dir: ./output/12-25/stageb_distill
  logging_dir: ./tb/12-25/stageb_distill
  run_name: epoch_2-stageb_distill-verdict
  # Conservative LR to avoid overwriting summary/dense skills while restoring Stage-B verdict behavior.
  learning_rate: 5.0e-5
  vit_lr: 1.0e-7
  aligner_lr: 1.0e-7
  save_delay_steps: 200

custom:
  # Dummy base path for ROOT_IMAGE_DIR auto-derivation; for fusion runs this is not used as the dataset.
  train_jsonl: data/stage_b/distill.train.jsonl
  val_jsonl: data/stage_b/distill.val.jsonl
  fusion_config: configs/fusion/stage_b_distill.yaml
  use_summary: false
  dump_conversation_text: false

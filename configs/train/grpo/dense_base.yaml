extends:
  - ../../base.yaml
  - ../../components/global/max_length_16000.yaml
  - ../../components/tuner/dora_full.yaml
  - ../../components/training/sft_defaults.yaml
  - ../../components/training/grpo_summary.yaml
  - ../../components/data/loader_defaults.yaml
  - ../../components/prompts/summary_runtime_bbu.yaml
  - ../../components/custom/common_fusion.yaml
  - ../../components/custom/assistant_prefix.yaml
  - ../../components/custom/augmentation_dense.yaml
  - ../../components/deepspeed/zero3.yaml
  - ../../components/rlhf/dense_grpo.yaml

model:
  model: null

custom:
  # Dense-only: keep default dense.
  use_summary: false
  val_sample_limit: 240
  # Optional: CHORD-style SFT mixing for GRPO (supervised fallback signal).
  # Uses the same (fusion) training dataset stream as expert data.
  grpo:
    dump:
      enabled: true
      # Analogous to logging_steps: dump every N global steps.
      dump_step: 40
      # Dump the most recent rollouts (prompt/rollout/GT).
      dump_sample_size: 16
      # Special value: reuse training.logging_dir (tensorboard dir for this run).
      dump_dir: logging_dir
      # Dump the full eval subset (bounded by custom.val_sample_limit in Qwen3-VL).
      eval_sample_size: val_sample_limit
    chord:
      enabled: false
      # Keep SFT micro-batch minimal; tuned for GPUs that can only backprop one sample per device.
      sft_per_device_train_batch_size: 1
      # Conservative constant mu after warmup: ramps to 0.05 then stays.
      mu_warmup_steps: 100
      mu_decay_steps: 0
      mu_peak: 0.05
      mu_valley: 0.05
      enable_phi_function: false

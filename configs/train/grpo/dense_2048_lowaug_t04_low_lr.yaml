extends:
  - dense_2048.yaml

training:
  # Lower policy update aggressiveness (LLM + aligner).
  # Note: logged `learning_rate` often corresponds to the first optimizer param-group (may be ViT),
  # so don't rely on logs to confirm LLM/aligner LR without inspecting optimizer groups.
  run_name: epoch_3-ebs32-gbs32-temp0.4-beta0.3-llm_lr-4e-6-vit_lr-2e-7-aligner_lr-7.5e-6-aug_off-doraLast12VIT-last8LLM-alignerMLP
  learning_rate: 4.0e-6
  aligner_lr: 7.5e-6

rlhf:
  # Reduce rollout variance / KL drift.
  temperature: 0.4
  top_p: 0.9
  beta: 0.3

tuner:
  # DoRA placement policy for post-GRPO dense runs:
  # - Always adapt aligner MLPs (merger + deepstack_merger_list.*)
  # - Adapt the later half of ViT blocks (12-23 for depth=24) => "last 12"
  # - Adapt the last 8 LLM blocks (28-35 for num_hidden_layers=36)
  #
  # NOTE: this regex matches MODULE NAMES (not parameter names).
  # Keep this as a SINGLE LINE (no YAML folding), otherwise whitespace becomes part of the regex
  # and LoRA/DoRA injection can silently match nothing.
  target_regex: '^(model\.visual\.merger\.(linear_fc1|linear_fc2)|model\.visual\.deepstack_merger_list\.(0|1|2)\.(linear_fc1|linear_fc2)|model\.visual\.blocks\.(1[2-9]|2[0-3])\.(attn\.(qkv|proj)|mlp\.(linear_fc1|linear_fc2))|model\.language_model\.layers\.(2[8-9]|3[0-5])\.(self_attn\.(q_proj|k_proj|v_proj|o_proj)|mlp\.(gate_proj|up_proj|down_proj)))$'

custom:
  # Disable augmentation at the experiment layer (keep defaults visible in `dense_2048.yaml`).
  object_ordering_policy: center_tlbr
  augmentation:
    enabled: false
    bypass_prob: 1.0
    ops: []
  augmentation_curriculum: null

training:
  # Baseline dense-caption SFT defaults.
  output_dir: ./output/12-27/new_schema-4B-dense
  run_name: epoch_30-2_1_6
  num_train_epochs: 30
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  effective_batch_size: 32
  optimizer: multimodal
  # LR groups (LLM/ViT/aligner) â€” tune per experiment as needed.
  learning_rate: 2.0e-4
  vit_lr: 1.0e-4
  aligner_lr: 6.0e-4
  lr_scheduler_type: cosine_warmup_with_min_lr
  lr_scheduler_kwargs:
    min_lr: 1.0e-6
  warmup_ratio: 0.1
  gradient_checkpointing: true
  bf16: true
  report_to: [tensorboard]
  seed: 17
  max_grad_norm: 0.5
  weight_decay: 0.0
  packing: false
  add_version: true
  save_safetensors: true
  eval_strategy: steps
  eval_steps: 40
  save_strategy: best
  save_steps: 40
  save_total_limit: 2
  save_last_epoch: true
  save_only_model: true
  metric_for_best_model: eval_token_acc
  greater_is_better: true
  save_delay_steps: 1800
  logging_dir: ./tb/12-27/new_schema-4B
  logging_steps: 10
  logging_first_step: true

rlhf:
  rlhf_type: grpo
  reward_funcs: [summary.format, summary.header, summary.strict, summary.parse, summary.no_dup_keys, summary.dataset, summary.category_recall, summary.content_structured_tversky, summary.text_bbu, summary.notes_bbu, summary.group_stats_presence]
  reward_weights: [1.0, 2.0, 2.0, 1.5, 2.0, 0.5, 0.5, 1.8, 0.8, 0.3, 0.2]
  # Make core GRPO knobs explicit (avoid relying on library defaults).
  beta: 0.04
  # Prefer ms-swift>=3.10 "AdvancedResearch" variants over vanilla GRPO:
  # - rloo: unbiased leave-one-out baseline (scale_rewards=none, kl_in_reward=true)
  # - reinforce_plus_plus: REINFORCE++ Baseline for outcome rewards (scale_rewards=batch, kl_in_reward=true)
  #
  # Current choice: REINFORCE++ Baseline.
  advantage_estimator: reinforce_plus_plus
  scale_rewards: batch
  kl_in_reward: true
  num_generations: 4
  temperature: 0.7
  repetition_penalty: 1.05
  max_completion_length: 2048
  truncation_strategy: delete
  generation_batch_size: 24
  # ref model is not allowed for Lora training
  dynamic_sample: true
  max_resample_times: 1

  # vLLM Configuration - Colocate Mode (internal vLLM)
  use_vllm: true
  vllm_mode: colocate
  vllm_gpu_memory_utilization: 0.53
  vllm_enable_prefix_caching: true
  vllm_enable_lora: false
  # async_generate: true

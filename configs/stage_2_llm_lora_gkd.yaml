extends: base.yaml

# Stage 2: LoRA on LLM last-4 blocks, freeze ViT; fully train MLP aligner (TOON + GKD)

model:
  model: output/11-07/stage_1-sft/v0-20251107-073037/eff_batch_64-epoch_10/checkpoint-200

tuner:
  train_type: lora
  use_swift_lora: false  # Use PEFT backend for better compatibility
  freeze_llm: false      # Enable LoRA on LLM last-4 layers (see target_regex below)
  freeze_vit: true       # Freeze vision tower completely
  freeze_aligner: false  # Fully tune aligner modules alongside LoRA
  target_modules: [all-linear]
  # Apply LoRA to last 4 LLM layers (32-35) for the 8B model (which has 36 layers total, 0-35)
  # This regex targets language_model.layers.{32,33,34,35}.{self_attn,mlp}.*
  target_regex: '^(?:model\.)?language_model\.layers\.(?:32|33|34|35)\.(?:self_attn\.(?:q_proj|k_proj|v_proj|o_proj)|mlp\.(?:gate_proj|up_proj|down_proj))$'
  # CRITICAL: modules_to_save does NOT support ModuleList - must specify individual elements
  # Qwen3-VL has: model.visual.merger (single module) + deepstack_merger_list (ModuleList with 3 elements)
  # These modules will be FULLY fine-tuned (not LoRA)
  modules_to_save:
    - model.visual.merger
    - model.visual.deepstack_merger_list.0
    - model.visual.deepstack_merger_list.1
    - model.visual.deepstack_merger_list.2
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
  lora_bias: none

rlhf:
  rlhf_type: gkd
  teacher_model: model_cache/models/Qwen/Qwen3-VL-8B-Instruct
  sft_alpha: 1
  llm_kd_weight: 0.1  # Example: down-weight LM KD while keeping visual KD enabled.
  seq_kd: false
  lmbda: 0.0

training:
  # Paths and naming
  output_dir: ./output/11-08/stage_2_llm_lora_gkd
  run_name: lr_5e-4-eff_batch_32-last_4-kl_weights_0.1_0.5-epoch_10
  
  # Epochs and batch sizes
  num_train_epochs: 10
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  
  # Optimizer and learning rates
  optimizer: multimodal
  learning_rate: 5.0e-4
  vit_lr: 2.0e-5
  aligner_lr: 2.0e-4
  
  # Eval and checkpointing
  eval_strategy: steps
  save_strategy: best
  eval_steps: 20
  save_steps: 100
  save_total_limit: 4
  save_delay_steps: 200
  save_only_model: true
  
  # Logging
  logging_dir: ./tb/11-08/stage_2
  logging_steps: 10
  logging_first_step: true

data:
  dataset: [placeholder]
  dataset_num_proc: 8
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4
  eval_dataset: [placeholder]

custom:
  trainer_variant: gkd_monitor
  visual_kd:
    enabled: true
    weight: 0.5  
    targets: [merger, deepstack]
    distance: mse
  emit_norm: norm1000
  dump_conversation_text: false
